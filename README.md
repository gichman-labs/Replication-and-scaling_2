# Домашнее задание к занятию "`Репликация и масштабирование. Часть 2`" - `Oleg Avvakumov`

### Задание 1

`Опишите основные преимущества использования масштабирования методами:
активный master-сервер и пассивный репликационный slave-сервер;
master-сервер и несколько slave-серверов;
Дайте ответ в свободной форме.`

### Ответ

'1) Активный master и пассивный репликационный slave

Отказоустойчивость (страховка на случай падения мастера).
Если мастер выходит из строя, у нас уже есть актуальная копия данных на slave, и его можно поднять как новый основной сервер. Это ровно тот “паттерн применения”, который упоминается в материалах: много копий данных → проще пережить отказ.     Разгрузка мастера от части задач (особенно чтения). Мастер можно держать “для записи и критичных операций”, а на slave отправлять чтение, отчёты, аналитику, тяжёлые SELECT’ы — чтобы они не мешали транзакциям записи на мастере. По сути это даёт более стабильную работу при росте нагрузки.

Бэкапы удобнее делать со slave.
Снимать дампы/резервные копии с реплики обычно менее болезненно: мастер меньше проседает по производительности, потому что его не нагружаем “сервисными” операциями.

Упрощение обслуживания.
На slave можно тестировать часть изменений (например, настройки чтения, мониторинг), проводить плановые работы, не трогая мастер напрямую.

2) Master и несколько slave-серверов

Масштабирование чтения почти “в ширину”.
Если запросов на чтение становится много (витрина, каталог, отчёты, API), то несколько slave позволяют распределить read-нагрузку между ними. Это логично продолжает идею горизонтального масштабирования: добавили узлы → выросла суммарная пропускная способность. 

Разделение ролей между репликами.
Можно выделить:

один slave — под отчёты/аналитику (тяжёлые запросы),

второй — под бэкапы,

третий — под чтение приложения.
Так мы уменьшаем влияние “тяжёлых” задач на пользовательские запросы.

Больше вариантов для отказоустойчивости.
Когда реплик несколько, при проблемах с одной репликой сервис всё равно продолжает работать на остальных (хотя мастер остаётся единой точкой записи). А если проблема у мастера — есть выбор, кого поднимать как нового мастера. Смысл всё тот же: много копий данных повышает живучесть системы. 

География и восстановление после аварий.
Реплики можно держать в разных зонах/машинах (даже в другом ДЦ), чтобы проще переживать проблемы уровня “упал хост/сеть/площадка”. Это практическое развитие идеи “копий данных” для устойчивости. 


 Если система небольшая и основная цель — подстраховаться на случай падения мастера, то чаще всего хватает схемы master + 1 пассивный slave: она простая, даёт копию данных и позволяет относительно быстро восстановиться, а также удобно переносить на реплику бэкапы и тяжёлые чтения. 
 А когда проект растёт и появляется много запросов на чтение (витрины, API, отчёты), тогда логично переходить на master + несколько slave: так чтение можно распределить по репликам, выделить отдельные узлы под аналитику/бэкапы и в целом повысить устойчивость к сбоям отдельных реплик, сохранив мастера как единую точку записи.

'


---

### Задание 2
'Разработайте план для выполнения горизонтального и вертикального шаринга базы данных. База данных состоит из трёх таблиц:

пользователи,
книги,
магазины (столбцы произвольно).

Опишите принципы построения системы и их разграничение или разбивку между базами данных.

Пришлите блоксхему, где и что будет располагаться. Опишите, в каких режимах будут работать сервера.'

### Ответ

`
1) Вертикальный шардинг (делим по “смыслам” и частоте использования)

Идея: вынести разные группы столбцов в отдельные базы/сервера, чтобы чаще читаемые данные не тянули за собой “тяжёлые” или чувствительные поля. В конспекте вертикальный шардинг показан на примере users, когда логин/имя отделяют от пароля и т.п.

Я бы сделал так:
users разделил на:
users_profile(id, username, email, phone, created_at, status) — профиль и общие данные (часто читаются),
users_security(user_id, password_hash, 2fa_secret, security_flags, last_login) — безопасность (реже читается, доступ строже).

books разделил на:
books_catalog(id, title, author, category_id, year, price, isbn) — каталог/поиск,
book_content(book_id, description, cover, file_link) — “тяжёлое” описание/контент.

shops разделил на:
shops_main(id, region, city, address, work_hours, status) — основные поля,
shops_extra(shop_id, geo_lat, geo_lon, photos, tags, meta) — расширенные данные.

2) Горизонтальный шардинг (делим по строкам)

Идея: когда записей много, делим строки на диапазоны или по ключу. В лекции пример горизонтального шардинга показан как разбиение по диапазонам id (например 0–500000 и 500001–1000000).

Я бы выбрал такие ключи:
users — по id (по диапазонам или id % N).

books — по category_id (логично, потому что запросы по книгам часто идут по категории).

shops — по region/city (география — естественный ключ).

Чтобы приложение не “думало”, в какой шард писать, нужен координатор/роутер: он даёт единое представление через VIEW (объединение UNION ALL) и правила для вставки, чтобы INSERT автоматически уходил в нужный шард.
На каждом шарде дополнительно задаются CHECK-ограничения на диапазон ключа и индекс по ключу шардирования.

3) Режимы работы серверов (master/slave)

Для надёжности и чтения я бы внутри каждого шарда применил репликацию master/slave: запись идёт на master, а чтение можно отдавать со slave (или нескольких slave). Это соответствует модели “активный master и пассивный репликационный slave” и “master + несколько slave”.`
'
                           ┌───────────────────┐
                           │    Приложение     │
                           └─────────┬─────────┘
                                     │
                                  v
             ┌────────────────────────────────────────────────────┐
             │ DB-Coordinator / Router (АКТИВНЫЙ)                 │
             │ • VIEW (UNION ALL) — единое представление данных   │
             │ • RULE — маршрутизация INSERT в нужный Shard       │
             └───────┬──────────────────┬──────────────────┬──────┘
                     │                  │                  │
                     v                  v                  v

┌─────────────────────────┐  ┌─────────────────────────┐  ┌─────────────────────────┐
│       USERS             │  │        BOOKS            │  │        SHOPS            │
│ (вертикальный шардинг)  │  │ (вертикальный шардинг)  │  │ (вертикальный шардинг)  │
└─────────┬───────────────┘  └─────────┬───────────────┘  └─────────┬───────────────┘
          │                            │                            │
          v                            v                            v

USERS_Profile (по столбцам)       BOOKS_Catalog (по столбцам)     SHOPS_Main (по столбцам)
  ├─ Shard 1: UProf_1 (M) ─▶ UProf_1 (S)   ├─ Shard 1: BC_1  (M) ─▶ BC_1  (S)   ├─ Shard 1: SM_R1 (M) ─▶ SM_R1 (S)
  └─ Shard 2: UProf_2 (M) ─▶ UProf_2 (S)   └─ Shard 2: BC_2  (M) ─▶ BC_2  (S)   └─ Shard 2: SM_R2 (M) ─▶ SM_R2 (S)

USERS_Security (по столбцам)      BOOKS_Content (по столбцам)     SHOPS_Extra (по столбцам)
  ├─ Shard 1: USec_1  (M) ─▶ USec_1  (S)   ├─ Shard 1: BCT_1 (M) ─▶ BCT_1 (S)   ├─ Shard 1: SE_R1 (M) ─▶ SE_R1 (S)
  └─ Shard 2: USec_2  (M) ─▶ USec_2  (S)   └─ Shard 2: BCT_2 (M) ─▶ BCT_2 (S)   └─ Shard 2: SE_R2 (M) ─▶ SE_R2 (S)

Легенда:
(M) = Master (WRITE) — активный узел шарда
(S) = Slave  (READ/резерв) — реплика шарда
Shard 1/2 = горизонтальный шардинг (разные строки/диапазоны/ключи)
                   
'


---


